{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ml-binary-sem-tratamento.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Treinos e Testes"
      ],
      "metadata": {
        "id": "r0z1LVkLcQPQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install validators;\n",
        "!pip install unidecode;"
      ],
      "metadata": {
        "id": "TuKgwSW5yJJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "kZAdUVSg0BcR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# from collections import Counter\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "import validators\n",
        "from urllib.parse import urlparse"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Le o CSV de classificadores\n",
        "classificacoes = pd.read_csv('covidbr_labeled.csv')\n",
        "textos = classificacoes['text']\n",
        "classes = classificacoes['misinformation']\n",
        "\n",
        "if len(textos) != len(classes):\n",
        "  print(f\"Inconsistência nos dados. {len(textos)} textos e {len(classes)} classificacoes.\")"
      ],
      "metadata": {
        "id": "X8qb6fxR0La5"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Carrega stopwords do arquivo\n",
        "stopwords = []\n",
        "with open('stopwords.txt') as f:\n",
        "  lines = f.readlines()\n",
        "  stopwords = list(map(lambda x: x.strip(), lines))\n",
        "\n",
        "# print(stopwords[:5])"
      ],
      "metadata": {
        "id": "fO6A0RrNERs9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizacao\n",
        "import unidecode\n",
        "\n",
        "def tokeniza(xs):\n",
        "  # Remove vírgulas\n",
        "  nxs = [ str(texto).replace(',', ' ') for texto in xs]  \n",
        "\n",
        "  # Remove acentos e capitalização e quebra os textos em palavras\n",
        "  n2xs = [unidecode.unidecode(texto).lower().split() for texto in nxs]\n",
        "\n",
        "  return n2xs\n",
        "\n",
        "textos = tokeniza(textos)\n",
        "\n",
        "if len(textos) != len(classes):\n",
        "  print(f\"Inconsistência nos dados. {len(textos)} textos e {len(classes)} classificacoes.\")"
      ],
      "metadata": {
        "id": "rxKVsDiev_gP"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tratamento inicial das palavras\n",
        "\n",
        "def trata_textos(xs):\n",
        "\n",
        "  newTextos = []\n",
        "  for lista in xs:\n",
        "    if not isinstance(lista, list): continue\n",
        "    newLista = []\n",
        "    for palavra in lista:\n",
        "\n",
        "      # Remove palavras vazias ou menores que 3 caracteres\n",
        "      if len(palavra) <= 3: continue\n",
        "\n",
        "      # Remove nomes de usuário\n",
        "      if palavra.startswith('@'): continue\n",
        "\n",
        "      # Remove stopwords:\n",
        "      if palavra in stopwords: continue\n",
        "\n",
        "      # Usa apenas o hostname das URLs\n",
        "      if validators.url(palavra): palavra = urlparse(palavra).hostname\n",
        "\n",
        "      # Remove pontos e espaços em branco\n",
        "      palavra = palavra.replace('.', ' ').replace('?', ' ').replace(';', ' ').replace('!', ' ').replace(':', ' ').strip().split()\n",
        "\n",
        "      for p in palavra:\n",
        "        if not p.isnumeric(): newLista.append(p)\n",
        "\n",
        "    newTextos.append(newLista)\n",
        "  return newTextos\n",
        "\n",
        "textos = trata_textos(textos)\n",
        "\n",
        "if len(textos) != len(classes):\n",
        "  print(f\"Inconsistência nos dados. {len(textos)} textos e {len(classes)} classificacoes.\")"
      ],
      "metadata": {
        "id": "7Xu-5fbKvj3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cria um conjunto com todas as palavras sem repetição\n",
        "palavras = { palavra for lista in textos for palavra in lista }\n",
        "\n",
        "# Associa um ID a cada palavra\n",
        "palavras = { palavra: i for i, palavra in enumerate(palavras) }"
      ],
      "metadata": {
        "id": "pc6nrBOL0PNE"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Conta a presenca de cada palavra num texto\n",
        "def contaPalavras(texto, palavras):\n",
        "    vetor = [0] * len(palavras)\n",
        "    for palavra in texto:\n",
        "        if palavra in palavras:\n",
        "            posicao = palavras[palavra]\n",
        "            vetor[posicao] += 1\n",
        "    return vetor\n",
        "\n",
        "vetoresDeTexto = [contaPalavras(texto, palavras) for texto in textos]"
      ],
      "metadata": {
        "id": "UXL88Hdj0Pv-"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Armazena o texto e as classes\n",
        "x = np.array(vetoresDeTexto)\n",
        "y = np.array(classes)\n",
        "\n",
        "if x.shape[0] != y.shape[0]:\n",
        "  print(f\"Inconsistência nos dados. {x.shape[0]} textos e {y.shape[0]} classificacoes.\")"
      ],
      "metadata": {
        "id": "CiAGVIvO0UmY"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define porcentagem de treino\n",
        "porcentagem_de_treino = 0.8\n",
        "\n",
        "# Define o tamanho dos dados de treino\n",
        "tamanho_de_treino = int(porcentagem_de_treino * len(y))\n",
        "tamanho_de_validacao = len(y) - tamanho_de_treino\n",
        "\n",
        "# Pega os dados de treino\n",
        "treino_dados = x[:tamanho_de_treino]\n",
        "treino_marcacoes = y[:tamanho_de_treino]\n",
        "\n",
        "# Pega os dados de validacao\n",
        "validacao_dados = x[tamanho_de_treino:]\n",
        "validacao_marcacoes = y[tamanho_de_treino:]"
      ],
      "metadata": {
        "id": "YxuKVjfZ0VLo"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Treina os dados\n",
        "def predict(modelo, treino_dados, treino_marcacoes):\n",
        "    k = 10\n",
        "    scores = cross_val_score(modelo, treino_dados, treino_marcacoes, cv = k, n_jobs = -1, verbose=20)\n",
        "    taxa_de_acerto = np.mean(scores)\n",
        "    msg = f\"Taxa de acerto: {taxa_de_acerto*100:.2f}%\"\n",
        "    print(msg)\n",
        "    return taxa_de_acerto"
      ],
      "metadata": {
        "id": "W_xBpwO50X8c"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Utilizando o classificador OneVsRest\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "resultados = {}\n",
        "modeloOneVsRest = OneVsRestClassifier(LinearSVC(random_state = 0))\n",
        "# resultadoOneVsRest = predict(modeloOneVsRest, treino_dados, treino_marcacoes)\n",
        "# resultados[resultadoOneVsRest] = modeloOneVsRest"
      ],
      "metadata": {
        "id": "gGPDCEuA0Zw1"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Utiliznado o classificador OneVsOne\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "modeloOneVsOne = OneVsOneClassifier(LinearSVC(random_state = 0))\n",
        "# resultadoOneVsOne = predict(modeloOneVsOne, treino_dados, treino_marcacoes)\n",
        "# resultados[resultadoOneVsOne] = modeloOneVsOne"
      ],
      "metadata": {
        "id": "CZRzJ8IK1oU1"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Utilizando o classificador MultinomialNb\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "modeloMultinomial = MultinomialNB()\n",
        "# resultadoMultinomial = predict(modeloMultinomial, treino_dados, treino_marcacoes)\n",
        "# resultados[resultadoMultinomial] = modeloMultinomial"
      ],
      "metadata": {
        "id": "FWq8aE601qzT"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Utilizando o classificador Adaboost\n",
        "resultados = {}\n",
        "modeloAdaBoost = AdaBoostClassifier()\n",
        "# resultadoAdaBoost = predict(modeloAdaBoost, treino_dados, treino_marcacoes)\n",
        "# resultados[resultadoAdaBoost] = modeloAdaBoost"
      ],
      "metadata": {
        "id": "7DWI4MTY1KXl"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center>Sem tratamento:</center>\n",
        "\n",
        "| %   | 1vs1  | 1vsR  | Multi | Ada   |\n",
        "|-----|-------|-------|-------|-------|\n",
        "| 0.5 | 73.70 | 73.70 | 74.59 | 71.49 |\n",
        "| 0.8 | 67.01 | 67.01 | 66.49 | 69.81 |\n",
        "| 1   | 65.73 | 65.73 | 65.42 | 69.68 |"
      ],
      "metadata": {
        "id": "o5elP7F-6bk0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center>Tratamento completo:</center>\n",
        "\n",
        "| %   | 1vs1  | 1vsR  | Multi | Ada   |\n",
        "|-----|-------|-------|-------|-------|\n",
        "| 0.5 |       |       |       |       |\n",
        "| 0.8 | 84.13 | 84.13 | 84.86 | 67.96 |\n",
        "| 1   | 84.17 | 84.17 | 84.67 | 81.91 |"
      ],
      "metadata": {
        "id": "kGLI5nKrGWe7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modeloMultinomial.fit(x, y)\n",
        "modeloOneVsOne.fit(x, y)\n",
        "modeloAdaBoost.fit(x, y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkCphizoOYDQ",
        "outputId": "0d2320b4-905d-4e75-e36b-a0a3552ba7a4"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AdaBoostClassifier()"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sc1 = modeloMultinomial.score(validacao_dados, validacao_marcacoes)\n",
        "sc2 = modeloAdaBoost.score(validacao_dados, validacao_marcacoes)\n",
        "sc3 = modeloOneVsOne.score(validacao_dados, validacao_marcacoes)\n",
        "print(sc1, sc2, sc3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Tb4ElnmnSRG",
        "outputId": "248a3481-f0c4-4570-dab0-c61a479cf929"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8879310344827587 0.8379310344827586 0.9879310344827587\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classificação"
      ],
      "metadata": {
        "id": "J7Xi4N65aV5M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Le o dados para classificação\n",
        "conteudo = pd.read_csv('mensagens-covid.db.csv')\n",
        "\n",
        "# Utiliza uma amostra aleatória de 10%\n",
        "conteudo = conteudo.sample(frac=0.1)\n",
        "\n",
        "mensagens = conteudo['message']\n",
        "ids = list(zip(conteudo['id'],conteudo['channel_id']))\n",
        "\n",
        "if len(mensagens) != len(ids):\n",
        "  print(f\"Inconsistência nos dados. {len(mensagens)} mensagens e {len(ids)} ids.\")"
      ],
      "metadata": {
        "id": "xcKeHhHoYcF4"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokeniza as mensagens\n",
        "mensagens = tokeniza(mensagens)\n",
        "\n",
        "if len(mensagens) != len(ids):\n",
        "  print(f\"Inconsistência nos dados. {len(mensagens)} mensagens e {len(ids)} ids.\")"
      ],
      "metadata": {
        "id": "GntXU0dNcirU"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Trata as mensagens\n",
        "mensagens = trata_textos(mensagens)\n",
        "\n",
        "if len(mensagens) != len(ids):\n",
        "  print(f\"Inconsistência nos dados. {len(mensagens)} mensagens e {len(ids)} ids.\")"
      ],
      "metadata": {
        "id": "U8shrbOQcmvc"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(mensagens[:5])"
      ],
      "metadata": {
        "id": "cQ_UVlOygWao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Conta as palavras das mensagens\n",
        "vetoresDeTexto = [contaPalavras(texto, palavras) for texto in mensagens]\n",
        "\n",
        "# Armazena as mensagens e os ids\n",
        "x1 = np.array(vetoresDeTexto)\n",
        "y1 = np.array(ids)\n",
        "\n",
        "if x1.shape[0] != y1.shape[0]:\n",
        "  print(f\"Inconsistência nos dados. {x1.shape[0]} mensagens e {y1.shape[0]} ids.\")"
      ],
      "metadata": {
        "id": "Kdq-IQuEc2RI"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p1 = modeloOneVsOne.predict(x1)\n",
        "print(f\"1v1: {p1}\")\n",
        "p2 = modeloMultinomial.predict(x1)\n",
        "print(f\"Multi: {p2}\")\n",
        "p3 = modeloAdaBoost.predict(x1)\n",
        "print(f\"Ada: {p3}\")"
      ],
      "metadata": {
        "id": "f3qLYfwidWKc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4dac8267-132d-4ed5-e29d-8334a2397603"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1v1: [0 0 0 ... 0 0 0]\n",
            "Multi: [0 1 0 ... 0 1 1]\n",
            "Ada: [0 0 0 ... 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Gerando CSVs com resultados\n",
        "import csv\n",
        "\n",
        "def gera_csv(nome,arr):\n",
        "  with open(nome, 'w') as csvfile:\n",
        "      fieldnames = ['id', 'channel_id', 'message', 'misinformation']\n",
        "      writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "      writer.writeheader()\n",
        "      for item in list(zip(y1,conteudo['message'],arr)):\n",
        "        writer.writerow(\n",
        "            {'id': item[0][0],\n",
        "            'channel_id': item[0][1],\n",
        "            'message': item[1],\n",
        "            'misinformation': item[2]})\n",
        "        \n",
        "gera_csv('saida_1v1.csv',p1)\n",
        "gera_csv('saida_multi.csv',p2)\n",
        "gera_csv('saida_ada.csv',p3)"
      ],
      "metadata": {
        "id": "ME5wrqTdPq4X"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Análise dos Resultados"
      ],
      "metadata": {
        "id": "BOXsGFrGWmUI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lê os arquivos de resultado final\n",
        "result_1v1 = pd.read_csv('saida_1v1.csv')\n",
        "result_multi = pd.read_csv('saida_multi.csv')\n",
        "result_ada = pd.read_csv('saida_ada.csv')"
      ],
      "metadata": {
        "id": "UMq7QH6AWphM"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gera_csv_2(nome,df):\n",
        "  with open(nome, 'w') as csvfile:\n",
        "      fieldnames = ['id', 'channel_id', 'message', 'misinformation']\n",
        "      writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "      writer.writeheader()\n",
        "      for item in list(zip(y1,conteudo['message'],arr)):\n",
        "        writer.writerow(\n",
        "            {'id': item[0][0],\n",
        "            'channel_id': item[0][1],\n",
        "            'message': item[1],\n",
        "            'misinformation': item[2]})"
      ],
      "metadata": {
        "id": "qobO0P_nYULE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Recupera amostras aleatórias e salva em novos CSVs\n",
        "\n",
        "# 1v1:\n",
        "sample_1v1_pos = result_1v1.loc[result_1v1['misinformation'] == 1].sample(n=20)\n",
        "sample_1v1_neg = result_1v1.loc[result_1v1['misinformation'] == 0].sample(n=20)\n",
        "\n",
        "sample_1v1 = pd.concat([sample_1v1_pos,sample_1v1_neg])\n",
        "sample_1v1.to_csv('sample_1v1.csv')\n",
        "\n",
        "# Multi:\n",
        "sample_multi_pos = result_multi.loc[result_multi['misinformation'] == 1].sample(n=20)\n",
        "sample_multi_neg = result_multi.loc[result_multi['misinformation'] == 0].sample(n=20)\n",
        "\n",
        "sample_multi = pd.concat([sample_multi_pos,sample_multi_neg])\n",
        "sample_multi.to_csv('sample_multi.csv')\n",
        "\n",
        "# Ada:\n",
        "sample_ada_pos = result_ada.loc[result_ada['misinformation'] == 1].sample(n=20)\n",
        "sample_ada_neg = result_ada.loc[result_ada['misinformation'] == 0].sample(n=20)\n",
        "\n",
        "sample_ada = pd.concat([sample_ada_pos,sample_ada_neg])\n",
        "sample_ada.to_csv('sample_ada.csv')"
      ],
      "metadata": {
        "id": "D21ftnPQXM2A"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1v1\n",
        "40% de falsos positivos, 20% de falsos negativos\n",
        "\n",
        "## Multi\n",
        "10% de falsos positivos, 20% de falsos negativos\n",
        "\n",
        "## Ada\n",
        "40% de falsos positivos, 20% de falsos negativos\n"
      ],
      "metadata": {
        "id": "1BXoK3PMZdK3"
      }
    }
  ]
}